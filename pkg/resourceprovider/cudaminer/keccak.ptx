//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_52
.address_size 64

.extern .func  (.param .b64 func_retval0) malloc
(
	.param .b64 malloc_param_0
)
;
.extern .func free
(
	.param .b64 free_param_0
)
;
.const .align 8 .b8 CUDA_KECCAK_CONSTS[192] = {1, 0, 0, 0, 0, 0, 0, 0, 130, 128, 0, 0, 0, 0, 0, 0, 138, 128, 0, 0, 0, 0, 0, 128, 0, 128, 0, 128, 0, 0, 0, 128, 139, 128, 0, 0, 0, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 0, 129, 128, 0, 128, 0, 0, 0, 128, 9, 128, 0, 0, 0, 0, 0, 128, 138, 0, 0, 0, 0, 0, 0, 0, 136, 0, 0, 0, 0, 0, 0, 0, 9, 128, 0, 128, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 0, 139, 128, 0, 128, 0, 0, 0, 0, 139, 0, 0, 0, 0, 0, 0, 128, 137, 128, 0, 0, 0, 0, 0, 128, 3, 128, 0, 0, 0, 0, 0, 128, 2, 128, 0, 0, 0, 0, 0, 128, 128, 0, 0, 0, 0, 0, 0, 128, 10, 128, 0, 0, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 128, 129, 128, 0, 128, 0, 0, 0, 128, 128, 128, 0, 0, 0, 0, 0, 128, 1, 0, 0, 128, 0, 0, 0, 0, 8, 128, 0, 128, 0, 0, 0, 128};

.func  (.param .b32 func_retval0) _ZN39_INTERNAL_467e079b_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1_(
	.param .b64 _ZN39_INTERNAL_467e079b_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_0,
	.param .b64 _ZN39_INTERNAL_467e079b_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_1
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<10>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd9, [_ZN39_INTERNAL_467e079b_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_0];
	ld.param.u64 	%rd10, [_ZN39_INTERNAL_467e079b_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_1];
	cvta.to.global.u64 	%rd2, %rd10;
	cvta.to.local.u64 	%rd1, %rd9;
	ld.global.u64 	%rd3, [%rd2+24];
	ld.local.u64 	%rd4, [%rd1+24];
	setp.gt.u64 	%p1, %rd4, %rd3;
	mov.u16 	%rs3, 0;
	mov.u16 	%rs9, %rs3;
	@%p1 bra 	$L__BB0_7;

	setp.lt.u64 	%p2, %rd4, %rd3;
	mov.u16 	%rs4, 1;
	mov.u16 	%rs9, %rs4;
	@%p2 bra 	$L__BB0_7;

	ld.global.u64 	%rd5, [%rd2+16];
	ld.local.u64 	%rd6, [%rd1+16];
	setp.gt.u64 	%p3, %rd6, %rd5;
	mov.u16 	%rs9, %rs3;
	@%p3 bra 	$L__BB0_7;

	setp.lt.u64 	%p4, %rd6, %rd5;
	mov.u16 	%rs9, %rs4;
	@%p4 bra 	$L__BB0_7;

	ld.global.u64 	%rd7, [%rd2+8];
	ld.local.u64 	%rd8, [%rd1+8];
	setp.gt.u64 	%p5, %rd8, %rd7;
	mov.u16 	%rs9, %rs3;
	@%p5 bra 	$L__BB0_7;

	setp.lt.u64 	%p6, %rd8, %rd7;
	mov.u16 	%rs9, %rs4;
	@%p6 bra 	$L__BB0_7;

	ld.local.u64 	%rd11, [%rd1];
	ld.global.u64 	%rd12, [%rd2];
	setp.le.u64 	%p7, %rd11, %rd12;
	selp.u16 	%rs9, 1, 0, %p7;

$L__BB0_7:
	cvt.u32.u16 	%r1, %rs9;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	kernel_lilypad_pow
.visible .entry kernel_lilypad_pow(
	.param .u64 kernel_lilypad_pow_param_0,
	.param .u64 kernel_lilypad_pow_param_1,
	.param .u64 kernel_lilypad_pow_param_2,
	.param .u32 kernel_lilypad_pow_param_3,
	.param .u64 kernel_lilypad_pow_param_4
)
.maxntid 1024, 1, 1
.minnctapersm 1
{
	.local .align 16 .b8 	__local_depot1[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<43>;
	.reg .b32 	%r<80>;
	.reg .b64 	%rd<341>;


	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd66, [kernel_lilypad_pow_param_0];
	ld.param.u64 	%rd67, [kernel_lilypad_pow_param_1];
	ld.param.u64 	%rd68, [kernel_lilypad_pow_param_2];
	ld.param.u32 	%r4, [kernel_lilypad_pow_param_3];
	ld.param.u64 	%rd69, [kernel_lilypad_pow_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	setp.ge.u32 	%p1, %r1, %r4;
	@%p1 bra 	$L__BB1_7;

	cvta.to.global.u64 	%rd88, %rd67;
	cvt.u64.u32 	%rd89, %r1;
	mov.u64 	%rd90, 32;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd90;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd1, [retval0+0];
	} // callseq 0
	mov.u32 	%r79, 0;
	ld.global.u64 	%rd91, [%rd88];
	mov.u64 	%rd315, 0;
	add.s64 	%rd319, %rd91, %rd89;
	st.u64 	[%rd1], %rd319;
	ld.global.u64 	%rd92, [%rd88];
	setp.lt.u64 	%p2, %rd319, %rd92;
	selp.u64 	%rd93, 1, 0, %p2;
	ld.global.u64 	%rd94, [%rd88+8];
	add.s64 	%rd338, %rd94, %rd93;
	st.u64 	[%rd1+8], %rd338;
	ld.global.u64 	%rd95, [%rd88+8];
	setp.lt.u64 	%p3, %rd338, %rd95;
	selp.u64 	%rd96, 1, 0, %p3;
	ld.global.u64 	%rd97, [%rd88+16];
	add.s64 	%rd333, %rd97, %rd96;
	st.u64 	[%rd1+16], %rd333;
	ld.global.u64 	%rd98, [%rd88+16];
	setp.lt.u64 	%p4, %rd333, %rd98;
	selp.u64 	%rd99, 1, 0, %p4;
	ld.global.u64 	%rd100, [%rd88+24];
	add.s64 	%rd328, %rd100, %rd99;
	st.u64 	[%rd1+24], %rd328;
	cvta.to.global.u64 	%rd101, %rd66;
	ld.global.u8 	%rd102, [%rd101];
	ld.global.u8 	%rd103, [%rd101+1];
	bfi.b64 	%rd104, %rd103, %rd102, 8, 8;
	ld.global.u8 	%rd105, [%rd101+2];
	ld.global.u8 	%rd106, [%rd101+3];
	bfi.b64 	%rd107, %rd106, %rd105, 8, 8;
	bfi.b64 	%rd108, %rd107, %rd104, 16, 16;
	ld.global.u8 	%rd109, [%rd101+4];
	ld.global.u8 	%rd110, [%rd101+5];
	bfi.b64 	%rd111, %rd110, %rd109, 8, 8;
	ld.global.u8 	%rd112, [%rd101+6];
	ld.global.u8 	%rd113, [%rd101+7];
	bfi.b64 	%rd114, %rd113, %rd112, 8, 8;
	bfi.b64 	%rd115, %rd114, %rd111, 16, 16;
	bfi.b64 	%rd339, %rd115, %rd108, 32, 32;
	ld.global.u8 	%rd116, [%rd101+8];
	ld.global.u8 	%rd117, [%rd101+9];
	bfi.b64 	%rd118, %rd117, %rd116, 8, 8;
	ld.global.u8 	%rd119, [%rd101+10];
	ld.global.u8 	%rd120, [%rd101+11];
	bfi.b64 	%rd121, %rd120, %rd119, 8, 8;
	bfi.b64 	%rd122, %rd121, %rd118, 16, 16;
	ld.global.u8 	%rd123, [%rd101+12];
	ld.global.u8 	%rd124, [%rd101+13];
	bfi.b64 	%rd125, %rd124, %rd123, 8, 8;
	ld.global.u8 	%rd126, [%rd101+14];
	ld.global.u8 	%rd127, [%rd101+15];
	bfi.b64 	%rd128, %rd127, %rd126, 8, 8;
	bfi.b64 	%rd129, %rd128, %rd125, 16, 16;
	bfi.b64 	%rd334, %rd129, %rd122, 32, 32;
	ld.global.u8 	%rd130, [%rd101+16];
	ld.global.u8 	%rd131, [%rd101+17];
	bfi.b64 	%rd132, %rd131, %rd130, 8, 8;
	ld.global.u8 	%rd133, [%rd101+18];
	ld.global.u8 	%rd134, [%rd101+19];
	bfi.b64 	%rd135, %rd134, %rd133, 8, 8;
	bfi.b64 	%rd136, %rd135, %rd132, 16, 16;
	ld.global.u8 	%rd137, [%rd101+20];
	ld.global.u8 	%rd138, [%rd101+21];
	bfi.b64 	%rd139, %rd138, %rd137, 8, 8;
	ld.global.u8 	%rd140, [%rd101+22];
	ld.global.u8 	%rd141, [%rd101+23];
	bfi.b64 	%rd142, %rd141, %rd140, 8, 8;
	bfi.b64 	%rd143, %rd142, %rd139, 16, 16;
	bfi.b64 	%rd329, %rd143, %rd136, 32, 32;
	ld.global.u8 	%rd144, [%rd101+24];
	ld.global.u8 	%rd145, [%rd101+25];
	bfi.b64 	%rd146, %rd145, %rd144, 8, 8;
	ld.global.u8 	%rd147, [%rd101+26];
	ld.global.u8 	%rd148, [%rd101+27];
	bfi.b64 	%rd149, %rd148, %rd147, 8, 8;
	bfi.b64 	%rd150, %rd149, %rd146, 16, 16;
	ld.global.u8 	%rd151, [%rd101+28];
	ld.global.u8 	%rd152, [%rd101+29];
	bfi.b64 	%rd153, %rd152, %rd151, 8, 8;
	ld.global.u8 	%rd154, [%rd101+30];
	ld.global.u8 	%rd155, [%rd101+31];
	bfi.b64 	%rd156, %rd155, %rd154, 8, 8;
	bfi.b64 	%rd157, %rd156, %rd153, 16, 16;
	bfi.b64 	%rd324, %rd157, %rd150, 32, 32;
	add.u64 	%rd158, %SP, 0;
	add.u64 	%rd10, %SPL, 0;
	cvta.to.global.u64 	%rd11, %rd69;
	mov.u64 	%rd331, -9223372036854775808;
	mov.u64 	%rd323, 1;
	mov.u64 	%rd314, CUDA_KECCAK_CONSTS;
	mov.u64 	%rd316, %rd315;
	mov.u64 	%rd317, %rd315;
	mov.u64 	%rd318, %rd315;
	mov.u64 	%rd320, %rd315;
	mov.u64 	%rd321, %rd315;
	mov.u64 	%rd322, %rd315;
	mov.u64 	%rd325, %rd315;
	mov.u64 	%rd326, %rd315;
	mov.u64 	%rd327, %rd315;
	mov.u64 	%rd330, %rd315;
	mov.u64 	%rd332, %rd315;
	mov.u64 	%rd335, %rd315;
	mov.u64 	%rd336, %rd315;
	mov.u64 	%rd337, %rd315;

$L__BB1_2:
	xor.b64  	%rd217, %rd338, %rd339;
	xor.b64  	%rd218, %rd217, %rd337;
	xor.b64  	%rd219, %rd218, %rd336;
	xor.b64  	%rd168, %rd219, %rd335;
	xor.b64  	%rd220, %rd333, %rd334;
	xor.b64  	%rd221, %rd220, %rd332;
	xor.b64  	%rd222, %rd221, %rd331;
	xor.b64  	%rd160, %rd222, %rd330;
	xor.b64  	%rd223, %rd328, %rd329;
	xor.b64  	%rd224, %rd223, %rd327;
	xor.b64  	%rd225, %rd224, %rd326;
	xor.b64  	%rd162, %rd225, %rd325;
	xor.b64  	%rd226, %rd323, %rd324;
	xor.b64  	%rd227, %rd226, %rd322;
	xor.b64  	%rd228, %rd227, %rd321;
	xor.b64  	%rd164, %rd228, %rd320;
	xor.b64  	%rd229, %rd318, %rd319;
	xor.b64  	%rd230, %rd229, %rd317;
	xor.b64  	%rd231, %rd230, %rd316;
	xor.b64  	%rd166, %rd231, %rd315;
	mov.u32 	%r14, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd160;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd159, {vl,vh};
	@p  mov.b64 %rd159, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd232, %rd159, %rd166;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd162;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd161, {vl,vh};
	@p  mov.b64 %rd161, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd233, %rd161, %rd168;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd164;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd163, {vl,vh};
	@p  mov.b64 %rd163, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd234, %rd163, %rd160;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd166;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd165, {vl,vh};
	@p  mov.b64 %rd165, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd235, %rd165, %rd162;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd168;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd167, {vl,vh};
	@p  mov.b64 %rd167, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd236, %rd167, %rd164;
	xor.b64  	%rd237, %rd339, %rd232;
	xor.b64  	%rd204, %rd338, %rd232;
	xor.b64  	%rd216, %rd337, %rd232;
	xor.b64  	%rd192, %rd336, %rd232;
	xor.b64  	%rd180, %rd335, %rd232;
	xor.b64  	%rd170, %rd334, %rd233;
	xor.b64  	%rd172, %rd333, %rd233;
	xor.b64  	%rd212, %rd332, %rd233;
	xor.b64  	%rd202, %rd331, %rd233;
	xor.b64  	%rd198, %rd330, %rd233;
	xor.b64  	%rd182, %rd329, %rd234;
	xor.b64  	%rd214, %rd328, %rd234;
	xor.b64  	%rd184, %rd327, %rd234;
	xor.b64  	%rd210, %rd326, %rd234;
	xor.b64  	%rd176, %rd325, %rd234;
	xor.b64  	%rd206, %rd324, %rd235;
	xor.b64  	%rd200, %rd323, %rd235;
	xor.b64  	%rd186, %rd322, %rd235;
	xor.b64  	%rd208, %rd321, %rd235;
	xor.b64  	%rd190, %rd320, %rd235;
	xor.b64  	%rd194, %rd319, %rd236;
	xor.b64  	%rd174, %rd318, %rd236;
	xor.b64  	%rd178, %rd317, %rd236;
	xor.b64  	%rd188, %rd316, %rd236;
	xor.b64  	%rd196, %rd315, %rd236;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd170;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd169, {vl,vh};
	@p  mov.b64 %rd169, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r15, 44;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd172;
	shf.l.wrap.b32 vl, tl, th, %r15;
	shf.l.wrap.b32 vh, th, tl, %r15;
	setp.lt.u32 p, %r15, 32;
	@!p mov.b64 %rd171, {vl,vh};
	@p  mov.b64 %rd171, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r16, 20;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd174;
	shf.l.wrap.b32 vl, tl, th, %r16;
	shf.l.wrap.b32 vh, th, tl, %r16;
	setp.lt.u32 p, %r16, 32;
	@!p mov.b64 %rd173, {vl,vh};
	@p  mov.b64 %rd173, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r17, 61;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd176;
	shf.l.wrap.b32 vl, tl, th, %r17;
	shf.l.wrap.b32 vh, th, tl, %r17;
	setp.lt.u32 p, %r17, 32;
	@!p mov.b64 %rd175, {vl,vh};
	@p  mov.b64 %rd175, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r18, 39;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd178;
	shf.l.wrap.b32 vl, tl, th, %r18;
	shf.l.wrap.b32 vh, th, tl, %r18;
	setp.lt.u32 p, %r18, 32;
	@!p mov.b64 %rd177, {vl,vh};
	@p  mov.b64 %rd177, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r19, 18;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd180;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd179, {vl,vh};
	@p  mov.b64 %rd179, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r20, 62;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd182;
	shf.l.wrap.b32 vl, tl, th, %r20;
	shf.l.wrap.b32 vh, th, tl, %r20;
	setp.lt.u32 p, %r20, 32;
	@!p mov.b64 %rd181, {vl,vh};
	@p  mov.b64 %rd181, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r21, 43;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd184;
	shf.l.wrap.b32 vl, tl, th, %r21;
	shf.l.wrap.b32 vh, th, tl, %r21;
	setp.lt.u32 p, %r21, 32;
	@!p mov.b64 %rd183, {vl,vh};
	@p  mov.b64 %rd183, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r22, 25;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd186;
	shf.l.wrap.b32 vl, tl, th, %r22;
	shf.l.wrap.b32 vh, th, tl, %r22;
	setp.lt.u32 p, %r22, 32;
	@!p mov.b64 %rd185, {vl,vh};
	@p  mov.b64 %rd185, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r23, 8;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd188;
	shf.l.wrap.b32 vl, tl, th, %r23;
	shf.l.wrap.b32 vh, th, tl, %r23;
	setp.lt.u32 p, %r23, 32;
	@!p mov.b64 %rd187, {vl,vh};
	@p  mov.b64 %rd187, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r24, 56;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd190;
	shf.l.wrap.b32 vl, tl, th, %r24;
	shf.l.wrap.b32 vh, th, tl, %r24;
	setp.lt.u32 p, %r24, 32;
	@!p mov.b64 %rd189, {vl,vh};
	@p  mov.b64 %rd189, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r25, 41;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd192;
	shf.l.wrap.b32 vl, tl, th, %r25;
	shf.l.wrap.b32 vh, th, tl, %r25;
	setp.lt.u32 p, %r25, 32;
	@!p mov.b64 %rd191, {vl,vh};
	@p  mov.b64 %rd191, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r26, 27;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd194;
	shf.l.wrap.b32 vl, tl, th, %r26;
	shf.l.wrap.b32 vh, th, tl, %r26;
	setp.lt.u32 p, %r26, 32;
	@!p mov.b64 %rd193, {vl,vh};
	@p  mov.b64 %rd193, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r27, 14;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd196;
	shf.l.wrap.b32 vl, tl, th, %r27;
	shf.l.wrap.b32 vh, th, tl, %r27;
	setp.lt.u32 p, %r27, 32;
	@!p mov.b64 %rd195, {vl,vh};
	@p  mov.b64 %rd195, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r28, 2;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd198;
	shf.l.wrap.b32 vl, tl, th, %r28;
	shf.l.wrap.b32 vh, th, tl, %r28;
	setp.lt.u32 p, %r28, 32;
	@!p mov.b64 %rd197, {vl,vh};
	@p  mov.b64 %rd197, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r29, 55;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd200;
	shf.l.wrap.b32 vl, tl, th, %r29;
	shf.l.wrap.b32 vh, th, tl, %r29;
	setp.lt.u32 p, %r29, 32;
	@!p mov.b64 %rd199, {vl,vh};
	@p  mov.b64 %rd199, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r30, 45;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd202;
	shf.l.wrap.b32 vl, tl, th, %r30;
	shf.l.wrap.b32 vh, th, tl, %r30;
	setp.lt.u32 p, %r30, 32;
	@!p mov.b64 %rd201, {vl,vh};
	@p  mov.b64 %rd201, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r31, 36;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd204;
	shf.l.wrap.b32 vl, tl, th, %r31;
	shf.l.wrap.b32 vh, th, tl, %r31;
	setp.lt.u32 p, %r31, 32;
	@!p mov.b64 %rd203, {vl,vh};
	@p  mov.b64 %rd203, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r32, 28;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd206;
	shf.l.wrap.b32 vl, tl, th, %r32;
	shf.l.wrap.b32 vh, th, tl, %r32;
	setp.lt.u32 p, %r32, 32;
	@!p mov.b64 %rd205, {vl,vh};
	@p  mov.b64 %rd205, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r33, 21;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd208;
	shf.l.wrap.b32 vl, tl, th, %r33;
	shf.l.wrap.b32 vh, th, tl, %r33;
	setp.lt.u32 p, %r33, 32;
	@!p mov.b64 %rd207, {vl,vh};
	@p  mov.b64 %rd207, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r34, 15;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd210;
	shf.l.wrap.b32 vl, tl, th, %r34;
	shf.l.wrap.b32 vh, th, tl, %r34;
	setp.lt.u32 p, %r34, 32;
	@!p mov.b64 %rd209, {vl,vh};
	@p  mov.b64 %rd209, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r35, 10;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd212;
	shf.l.wrap.b32 vl, tl, th, %r35;
	shf.l.wrap.b32 vh, th, tl, %r35;
	setp.lt.u32 p, %r35, 32;
	@!p mov.b64 %rd211, {vl,vh};
	@p  mov.b64 %rd211, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r36, 6;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd214;
	shf.l.wrap.b32 vl, tl, th, %r36;
	shf.l.wrap.b32 vh, th, tl, %r36;
	setp.lt.u32 p, %r36, 32;
	@!p mov.b64 %rd213, {vl,vh};
	@p  mov.b64 %rd213, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r37, 3;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd216;
	shf.l.wrap.b32 vl, tl, th, %r37;
	shf.l.wrap.b32 vh, th, tl, %r37;
	setp.lt.u32 p, %r37, 32;
	@!p mov.b64 %rd215, {vl,vh};
	@p  mov.b64 %rd215, {vh,vl};
	}

	// end inline asm
	not.b64 	%rd238, %rd171;
	and.b64  	%rd239, %rd183, %rd238;
	xor.b64  	%rd240, %rd239, %rd237;
	not.b64 	%rd241, %rd183;
	and.b64  	%rd242, %rd207, %rd241;
	xor.b64  	%rd334, %rd242, %rd171;
	not.b64 	%rd243, %rd207;
	and.b64  	%rd244, %rd195, %rd243;
	xor.b64  	%rd329, %rd183, %rd244;
	not.b64 	%rd245, %rd195;
	and.b64  	%rd246, %rd237, %rd245;
	xor.b64  	%rd324, %rd207, %rd246;
	not.b64 	%rd247, %rd237;
	and.b64  	%rd248, %rd171, %rd247;
	xor.b64  	%rd319, %rd195, %rd248;
	not.b64 	%rd249, %rd173;
	and.b64  	%rd250, %rd215, %rd249;
	xor.b64  	%rd338, %rd250, %rd205;
	not.b64 	%rd251, %rd215;
	and.b64  	%rd252, %rd201, %rd251;
	xor.b64  	%rd333, %rd252, %rd173;
	not.b64 	%rd253, %rd201;
	and.b64  	%rd254, %rd175, %rd253;
	xor.b64  	%rd328, %rd215, %rd254;
	not.b64 	%rd255, %rd175;
	and.b64  	%rd256, %rd205, %rd255;
	xor.b64  	%rd323, %rd201, %rd256;
	not.b64 	%rd257, %rd205;
	and.b64  	%rd258, %rd173, %rd257;
	xor.b64  	%rd318, %rd175, %rd258;
	not.b64 	%rd259, %rd213;
	and.b64  	%rd260, %rd185, %rd259;
	xor.b64  	%rd337, %rd260, %rd169;
	not.b64 	%rd261, %rd185;
	and.b64  	%rd262, %rd187, %rd261;
	xor.b64  	%rd332, %rd262, %rd213;
	not.b64 	%rd263, %rd187;
	and.b64  	%rd264, %rd179, %rd263;
	xor.b64  	%rd327, %rd185, %rd264;
	not.b64 	%rd265, %rd179;
	and.b64  	%rd266, %rd169, %rd265;
	xor.b64  	%rd322, %rd187, %rd266;
	not.b64 	%rd267, %rd169;
	and.b64  	%rd268, %rd213, %rd267;
	xor.b64  	%rd317, %rd179, %rd268;
	not.b64 	%rd269, %rd203;
	and.b64  	%rd270, %rd211, %rd269;
	xor.b64  	%rd336, %rd270, %rd193;
	not.b64 	%rd271, %rd211;
	and.b64  	%rd272, %rd209, %rd271;
	xor.b64  	%rd331, %rd272, %rd203;
	not.b64 	%rd273, %rd209;
	and.b64  	%rd274, %rd189, %rd273;
	xor.b64  	%rd326, %rd211, %rd274;
	not.b64 	%rd275, %rd189;
	and.b64  	%rd276, %rd193, %rd275;
	xor.b64  	%rd321, %rd209, %rd276;
	not.b64 	%rd277, %rd193;
	and.b64  	%rd278, %rd203, %rd277;
	xor.b64  	%rd316, %rd189, %rd278;
	not.b64 	%rd279, %rd199;
	and.b64  	%rd280, %rd177, %rd279;
	xor.b64  	%rd335, %rd280, %rd181;
	not.b64 	%rd281, %rd177;
	and.b64  	%rd282, %rd191, %rd281;
	xor.b64  	%rd330, %rd282, %rd199;
	not.b64 	%rd283, %rd191;
	and.b64  	%rd284, %rd197, %rd283;
	xor.b64  	%rd325, %rd177, %rd284;
	not.b64 	%rd285, %rd197;
	and.b64  	%rd286, %rd181, %rd285;
	xor.b64  	%rd320, %rd191, %rd286;
	not.b64 	%rd287, %rd181;
	and.b64  	%rd288, %rd199, %rd287;
	xor.b64  	%rd315, %rd197, %rd288;
	ld.const.u64 	%rd289, [%rd314];
	xor.b64  	%rd339, %rd240, %rd289;
	add.s64 	%rd314, %rd314, 8;
	add.s32 	%r79, %r79, 1;
	setp.ne.s32 	%p5, %r79, 24;
	@%p5 bra 	$L__BB1_2;

	shr.u64 	%rd290, %rd339, 16;
	cvt.u32.u64 	%r38, %rd339;
	shr.u64 	%rd291, %rd339, 32;
	shr.u64 	%rd292, %rd339, 40;
	cvt.u32.u64 	%r39, %rd292;
	shr.u64 	%rd293, %rd339, 48;
	shr.u64 	%rd294, %rd339, 56;
	shr.u64 	%rd295, %rd334, 16;
	cvt.u32.u64 	%r40, %rd334;
	shr.u64 	%rd296, %rd334, 32;
	shr.u64 	%rd297, %rd334, 40;
	cvt.u32.u64 	%r41, %rd297;
	shr.u64 	%rd298, %rd334, 48;
	shr.u64 	%rd299, %rd334, 56;
	shr.u64 	%rd300, %rd329, 16;
	cvt.u32.u64 	%r42, %rd329;
	shr.u64 	%rd301, %rd329, 32;
	shr.u64 	%rd302, %rd329, 40;
	cvt.u32.u64 	%r43, %rd302;
	shr.u64 	%rd303, %rd329, 48;
	shr.u64 	%rd304, %rd329, 56;
	shr.u64 	%rd305, %rd324, 56;
	shr.u64 	%rd306, %rd324, 48;
	shr.u64 	%rd307, %rd324, 40;
	cvt.u32.u64 	%r44, %rd307;
	shr.u64 	%rd308, %rd324, 32;
	cvt.u32.u64 	%r45, %rd324;
	shr.u64 	%rd309, %rd324, 16;
	cvt.u16.u64 	%rs1, %rd305;
	cvt.u16.u64 	%rs2, %rd306;
	shl.b16 	%rs3, %rs2, 8;
	or.b16  	%rs4, %rs1, %rs3;
	cvt.u32.u64 	%r46, %rd308;
	and.b32  	%r47, %r44, 255;
	prmt.b32 	%r48, %r46, %r47, 30212;
	cvt.u16.u32 	%rs5, %r48;
	cvt.u16.u64 	%rs6, %rd304;
	cvt.u16.u64 	%rs7, %rd303;
	shl.b16 	%rs8, %rs7, 8;
	or.b16  	%rs9, %rs6, %rs8;
	cvt.u32.u64 	%r49, %rd301;
	and.b32  	%r50, %r43, 255;
	prmt.b32 	%r51, %r49, %r50, 30212;
	cvt.u16.u32 	%rs10, %r51;
	cvt.u16.u64 	%rs11, %rd324;
	shl.b16 	%rs12, %rs11, 8;
	shr.u16 	%rs13, %rs11, 8;
	or.b16  	%rs14, %rs13, %rs12;
	shr.u32 	%r52, %r45, 24;
	cvt.u32.u64 	%r53, %rd309;
	prmt.b32 	%r54, %r53, %r52, 30212;
	cvt.u16.u32 	%rs15, %r54;
	cvt.u16.u64 	%rs16, %rd329;
	shl.b16 	%rs17, %rs16, 8;
	shr.u16 	%rs18, %rs16, 8;
	or.b16  	%rs19, %rs18, %rs17;
	shr.u32 	%r55, %r42, 24;
	cvt.u32.u64 	%r56, %rd300;
	prmt.b32 	%r57, %r56, %r55, 30212;
	cvt.u16.u32 	%rs20, %r57;
	mov.b32 	%r58, {%rs20, %rs19};
	mov.b32 	%r59, {%rs15, %rs14};
	mov.b32 	%r60, {%rs9, %rs10};
	mov.b32 	%r61, {%rs4, %rs5};
	st.local.v4.u32 	[%rd10], {%r61, %r59, %r60, %r58};
	cvt.u16.u64 	%rs21, %rd299;
	cvt.u16.u64 	%rs22, %rd298;
	shl.b16 	%rs23, %rs22, 8;
	or.b16  	%rs24, %rs21, %rs23;
	cvt.u32.u64 	%r62, %rd296;
	and.b32  	%r63, %r41, 255;
	prmt.b32 	%r64, %r62, %r63, 30212;
	cvt.u16.u32 	%rs25, %r64;
	cvt.u16.u64 	%rs26, %rd294;
	cvt.u16.u64 	%rs27, %rd293;
	shl.b16 	%rs28, %rs27, 8;
	or.b16  	%rs29, %rs26, %rs28;
	cvt.u32.u64 	%r65, %rd291;
	and.b32  	%r66, %r39, 255;
	prmt.b32 	%r67, %r65, %r66, 30212;
	cvt.u16.u32 	%rs30, %r67;
	cvt.u16.u64 	%rs31, %rd334;
	shl.b16 	%rs32, %rs31, 8;
	shr.u16 	%rs33, %rs31, 8;
	or.b16  	%rs34, %rs33, %rs32;
	shr.u32 	%r68, %r40, 24;
	cvt.u32.u64 	%r69, %rd295;
	prmt.b32 	%r70, %r69, %r68, 30212;
	cvt.u16.u32 	%rs35, %r70;
	cvt.u16.u64 	%rs36, %rd339;
	shl.b16 	%rs37, %rs36, 8;
	shr.u16 	%rs38, %rs36, 8;
	or.b16  	%rs39, %rs38, %rs37;
	shr.u32 	%r71, %r38, 24;
	cvt.u32.u64 	%r72, %rd290;
	prmt.b32 	%r73, %r72, %r71, 30212;
	cvt.u16.u32 	%rs40, %r73;
	mov.b32 	%r74, {%rs40, %rs39};
	mov.b32 	%r75, {%rs35, %rs34};
	mov.b32 	%r76, {%rs29, %rs30};
	mov.b32 	%r77, {%rs24, %rs25};
	st.local.v4.u32 	[%rd10+16], {%r77, %r75, %r76, %r74};
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd158;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd68;
	.param .b32 retval0;
	call.uni (retval0), 
	_ZN39_INTERNAL_467e079b_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1_, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r78, [retval0+0];
	} // callseq 1
	cvt.u16.u32 	%rs41, %r78;
	setp.eq.s16 	%p6, %rs41, 0;
	@%p6 bra 	$L__BB1_6;

	mov.u64 	%rd340, 0;

$L__BB1_5:
	add.s64 	%rd312, %rd1, %rd340;
	ld.u8 	%rs42, [%rd312];
	add.s64 	%rd313, %rd11, %rd340;
	st.global.u8 	[%rd313], %rs42;
	add.s64 	%rd340, %rd340, 1;
	setp.lt.u64 	%p7, %rd340, 32;
	@%p7 bra 	$L__BB1_5;

$L__BB1_6:
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 2

$L__BB1_7:
	ret;

}
	// .globl	kernel_lilypad_pow_debug
.visible .entry kernel_lilypad_pow_debug(
	.param .u64 kernel_lilypad_pow_debug_param_0,
	.param .u64 kernel_lilypad_pow_debug_param_1,
	.param .u64 kernel_lilypad_pow_debug_param_2,
	.param .u32 kernel_lilypad_pow_debug_param_3,
	.param .u64 kernel_lilypad_pow_debug_param_4,
	.param .u64 kernel_lilypad_pow_debug_param_5,
	.param .u64 kernel_lilypad_pow_debug_param_6
)
.maxntid 1024, 1, 1
.minnctapersm 1
{
	.local .align 16 .b8 	__local_depot2[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<43>;
	.reg .b32 	%r<80>;
	.reg .b64 	%rd<341>;


	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd66, [kernel_lilypad_pow_debug_param_0];
	ld.param.u64 	%rd67, [kernel_lilypad_pow_debug_param_1];
	ld.param.u64 	%rd68, [kernel_lilypad_pow_debug_param_2];
	ld.param.u32 	%r4, [kernel_lilypad_pow_debug_param_3];
	ld.param.u64 	%rd69, [kernel_lilypad_pow_debug_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	setp.ge.u32 	%p1, %r1, %r4;
	@%p1 bra 	$L__BB2_7;

	cvta.to.global.u64 	%rd88, %rd67;
	cvt.u64.u32 	%rd89, %r1;
	mov.u64 	%rd90, 32;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd90;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd1, [retval0+0];
	} // callseq 3
	mov.u32 	%r79, 0;
	ld.global.u64 	%rd91, [%rd88];
	mov.u64 	%rd315, 0;
	add.s64 	%rd319, %rd91, %rd89;
	st.u64 	[%rd1], %rd319;
	ld.global.u64 	%rd92, [%rd88];
	setp.lt.u64 	%p2, %rd319, %rd92;
	selp.u64 	%rd93, 1, 0, %p2;
	ld.global.u64 	%rd94, [%rd88+8];
	add.s64 	%rd338, %rd94, %rd93;
	st.u64 	[%rd1+8], %rd338;
	ld.global.u64 	%rd95, [%rd88+8];
	setp.lt.u64 	%p3, %rd338, %rd95;
	selp.u64 	%rd96, 1, 0, %p3;
	ld.global.u64 	%rd97, [%rd88+16];
	add.s64 	%rd333, %rd97, %rd96;
	st.u64 	[%rd1+16], %rd333;
	ld.global.u64 	%rd98, [%rd88+16];
	setp.lt.u64 	%p4, %rd333, %rd98;
	selp.u64 	%rd99, 1, 0, %p4;
	ld.global.u64 	%rd100, [%rd88+24];
	add.s64 	%rd328, %rd100, %rd99;
	st.u64 	[%rd1+24], %rd328;
	cvta.to.global.u64 	%rd101, %rd66;
	ld.global.u8 	%rd102, [%rd101];
	ld.global.u8 	%rd103, [%rd101+1];
	bfi.b64 	%rd104, %rd103, %rd102, 8, 8;
	ld.global.u8 	%rd105, [%rd101+2];
	ld.global.u8 	%rd106, [%rd101+3];
	bfi.b64 	%rd107, %rd106, %rd105, 8, 8;
	bfi.b64 	%rd108, %rd107, %rd104, 16, 16;
	ld.global.u8 	%rd109, [%rd101+4];
	ld.global.u8 	%rd110, [%rd101+5];
	bfi.b64 	%rd111, %rd110, %rd109, 8, 8;
	ld.global.u8 	%rd112, [%rd101+6];
	ld.global.u8 	%rd113, [%rd101+7];
	bfi.b64 	%rd114, %rd113, %rd112, 8, 8;
	bfi.b64 	%rd115, %rd114, %rd111, 16, 16;
	bfi.b64 	%rd339, %rd115, %rd108, 32, 32;
	ld.global.u8 	%rd116, [%rd101+8];
	ld.global.u8 	%rd117, [%rd101+9];
	bfi.b64 	%rd118, %rd117, %rd116, 8, 8;
	ld.global.u8 	%rd119, [%rd101+10];
	ld.global.u8 	%rd120, [%rd101+11];
	bfi.b64 	%rd121, %rd120, %rd119, 8, 8;
	bfi.b64 	%rd122, %rd121, %rd118, 16, 16;
	ld.global.u8 	%rd123, [%rd101+12];
	ld.global.u8 	%rd124, [%rd101+13];
	bfi.b64 	%rd125, %rd124, %rd123, 8, 8;
	ld.global.u8 	%rd126, [%rd101+14];
	ld.global.u8 	%rd127, [%rd101+15];
	bfi.b64 	%rd128, %rd127, %rd126, 8, 8;
	bfi.b64 	%rd129, %rd128, %rd125, 16, 16;
	bfi.b64 	%rd334, %rd129, %rd122, 32, 32;
	ld.global.u8 	%rd130, [%rd101+16];
	ld.global.u8 	%rd131, [%rd101+17];
	bfi.b64 	%rd132, %rd131, %rd130, 8, 8;
	ld.global.u8 	%rd133, [%rd101+18];
	ld.global.u8 	%rd134, [%rd101+19];
	bfi.b64 	%rd135, %rd134, %rd133, 8, 8;
	bfi.b64 	%rd136, %rd135, %rd132, 16, 16;
	ld.global.u8 	%rd137, [%rd101+20];
	ld.global.u8 	%rd138, [%rd101+21];
	bfi.b64 	%rd139, %rd138, %rd137, 8, 8;
	ld.global.u8 	%rd140, [%rd101+22];
	ld.global.u8 	%rd141, [%rd101+23];
	bfi.b64 	%rd142, %rd141, %rd140, 8, 8;
	bfi.b64 	%rd143, %rd142, %rd139, 16, 16;
	bfi.b64 	%rd329, %rd143, %rd136, 32, 32;
	ld.global.u8 	%rd144, [%rd101+24];
	ld.global.u8 	%rd145, [%rd101+25];
	bfi.b64 	%rd146, %rd145, %rd144, 8, 8;
	ld.global.u8 	%rd147, [%rd101+26];
	ld.global.u8 	%rd148, [%rd101+27];
	bfi.b64 	%rd149, %rd148, %rd147, 8, 8;
	bfi.b64 	%rd150, %rd149, %rd146, 16, 16;
	ld.global.u8 	%rd151, [%rd101+28];
	ld.global.u8 	%rd152, [%rd101+29];
	bfi.b64 	%rd153, %rd152, %rd151, 8, 8;
	ld.global.u8 	%rd154, [%rd101+30];
	ld.global.u8 	%rd155, [%rd101+31];
	bfi.b64 	%rd156, %rd155, %rd154, 8, 8;
	bfi.b64 	%rd157, %rd156, %rd153, 16, 16;
	bfi.b64 	%rd324, %rd157, %rd150, 32, 32;
	add.u64 	%rd158, %SP, 0;
	add.u64 	%rd10, %SPL, 0;
	cvta.to.global.u64 	%rd11, %rd69;
	mov.u64 	%rd331, -9223372036854775808;
	mov.u64 	%rd323, 1;
	mov.u64 	%rd314, CUDA_KECCAK_CONSTS;
	mov.u64 	%rd316, %rd315;
	mov.u64 	%rd317, %rd315;
	mov.u64 	%rd318, %rd315;
	mov.u64 	%rd320, %rd315;
	mov.u64 	%rd321, %rd315;
	mov.u64 	%rd322, %rd315;
	mov.u64 	%rd325, %rd315;
	mov.u64 	%rd326, %rd315;
	mov.u64 	%rd327, %rd315;
	mov.u64 	%rd330, %rd315;
	mov.u64 	%rd332, %rd315;
	mov.u64 	%rd335, %rd315;
	mov.u64 	%rd336, %rd315;
	mov.u64 	%rd337, %rd315;

$L__BB2_2:
	xor.b64  	%rd217, %rd338, %rd339;
	xor.b64  	%rd218, %rd217, %rd337;
	xor.b64  	%rd219, %rd218, %rd336;
	xor.b64  	%rd168, %rd219, %rd335;
	xor.b64  	%rd220, %rd333, %rd334;
	xor.b64  	%rd221, %rd220, %rd332;
	xor.b64  	%rd222, %rd221, %rd331;
	xor.b64  	%rd160, %rd222, %rd330;
	xor.b64  	%rd223, %rd328, %rd329;
	xor.b64  	%rd224, %rd223, %rd327;
	xor.b64  	%rd225, %rd224, %rd326;
	xor.b64  	%rd162, %rd225, %rd325;
	xor.b64  	%rd226, %rd323, %rd324;
	xor.b64  	%rd227, %rd226, %rd322;
	xor.b64  	%rd228, %rd227, %rd321;
	xor.b64  	%rd164, %rd228, %rd320;
	xor.b64  	%rd229, %rd318, %rd319;
	xor.b64  	%rd230, %rd229, %rd317;
	xor.b64  	%rd231, %rd230, %rd316;
	xor.b64  	%rd166, %rd231, %rd315;
	mov.u32 	%r14, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd160;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd159, {vl,vh};
	@p  mov.b64 %rd159, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd232, %rd159, %rd166;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd162;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd161, {vl,vh};
	@p  mov.b64 %rd161, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd233, %rd161, %rd168;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd164;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd163, {vl,vh};
	@p  mov.b64 %rd163, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd234, %rd163, %rd160;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd166;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd165, {vl,vh};
	@p  mov.b64 %rd165, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd235, %rd165, %rd162;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd168;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd167, {vl,vh};
	@p  mov.b64 %rd167, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd236, %rd167, %rd164;
	xor.b64  	%rd237, %rd339, %rd232;
	xor.b64  	%rd204, %rd338, %rd232;
	xor.b64  	%rd216, %rd337, %rd232;
	xor.b64  	%rd192, %rd336, %rd232;
	xor.b64  	%rd180, %rd335, %rd232;
	xor.b64  	%rd170, %rd334, %rd233;
	xor.b64  	%rd172, %rd333, %rd233;
	xor.b64  	%rd212, %rd332, %rd233;
	xor.b64  	%rd202, %rd331, %rd233;
	xor.b64  	%rd198, %rd330, %rd233;
	xor.b64  	%rd182, %rd329, %rd234;
	xor.b64  	%rd214, %rd328, %rd234;
	xor.b64  	%rd184, %rd327, %rd234;
	xor.b64  	%rd210, %rd326, %rd234;
	xor.b64  	%rd176, %rd325, %rd234;
	xor.b64  	%rd206, %rd324, %rd235;
	xor.b64  	%rd200, %rd323, %rd235;
	xor.b64  	%rd186, %rd322, %rd235;
	xor.b64  	%rd208, %rd321, %rd235;
	xor.b64  	%rd190, %rd320, %rd235;
	xor.b64  	%rd194, %rd319, %rd236;
	xor.b64  	%rd174, %rd318, %rd236;
	xor.b64  	%rd178, %rd317, %rd236;
	xor.b64  	%rd188, %rd316, %rd236;
	xor.b64  	%rd196, %rd315, %rd236;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd170;
	shf.l.wrap.b32 vl, tl, th, %r14;
	shf.l.wrap.b32 vh, th, tl, %r14;
	setp.lt.u32 p, %r14, 32;
	@!p mov.b64 %rd169, {vl,vh};
	@p  mov.b64 %rd169, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r15, 44;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd172;
	shf.l.wrap.b32 vl, tl, th, %r15;
	shf.l.wrap.b32 vh, th, tl, %r15;
	setp.lt.u32 p, %r15, 32;
	@!p mov.b64 %rd171, {vl,vh};
	@p  mov.b64 %rd171, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r16, 20;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd174;
	shf.l.wrap.b32 vl, tl, th, %r16;
	shf.l.wrap.b32 vh, th, tl, %r16;
	setp.lt.u32 p, %r16, 32;
	@!p mov.b64 %rd173, {vl,vh};
	@p  mov.b64 %rd173, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r17, 61;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd176;
	shf.l.wrap.b32 vl, tl, th, %r17;
	shf.l.wrap.b32 vh, th, tl, %r17;
	setp.lt.u32 p, %r17, 32;
	@!p mov.b64 %rd175, {vl,vh};
	@p  mov.b64 %rd175, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r18, 39;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd178;
	shf.l.wrap.b32 vl, tl, th, %r18;
	shf.l.wrap.b32 vh, th, tl, %r18;
	setp.lt.u32 p, %r18, 32;
	@!p mov.b64 %rd177, {vl,vh};
	@p  mov.b64 %rd177, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r19, 18;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd180;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd179, {vl,vh};
	@p  mov.b64 %rd179, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r20, 62;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd182;
	shf.l.wrap.b32 vl, tl, th, %r20;
	shf.l.wrap.b32 vh, th, tl, %r20;
	setp.lt.u32 p, %r20, 32;
	@!p mov.b64 %rd181, {vl,vh};
	@p  mov.b64 %rd181, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r21, 43;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd184;
	shf.l.wrap.b32 vl, tl, th, %r21;
	shf.l.wrap.b32 vh, th, tl, %r21;
	setp.lt.u32 p, %r21, 32;
	@!p mov.b64 %rd183, {vl,vh};
	@p  mov.b64 %rd183, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r22, 25;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd186;
	shf.l.wrap.b32 vl, tl, th, %r22;
	shf.l.wrap.b32 vh, th, tl, %r22;
	setp.lt.u32 p, %r22, 32;
	@!p mov.b64 %rd185, {vl,vh};
	@p  mov.b64 %rd185, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r23, 8;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd188;
	shf.l.wrap.b32 vl, tl, th, %r23;
	shf.l.wrap.b32 vh, th, tl, %r23;
	setp.lt.u32 p, %r23, 32;
	@!p mov.b64 %rd187, {vl,vh};
	@p  mov.b64 %rd187, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r24, 56;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd190;
	shf.l.wrap.b32 vl, tl, th, %r24;
	shf.l.wrap.b32 vh, th, tl, %r24;
	setp.lt.u32 p, %r24, 32;
	@!p mov.b64 %rd189, {vl,vh};
	@p  mov.b64 %rd189, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r25, 41;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd192;
	shf.l.wrap.b32 vl, tl, th, %r25;
	shf.l.wrap.b32 vh, th, tl, %r25;
	setp.lt.u32 p, %r25, 32;
	@!p mov.b64 %rd191, {vl,vh};
	@p  mov.b64 %rd191, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r26, 27;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd194;
	shf.l.wrap.b32 vl, tl, th, %r26;
	shf.l.wrap.b32 vh, th, tl, %r26;
	setp.lt.u32 p, %r26, 32;
	@!p mov.b64 %rd193, {vl,vh};
	@p  mov.b64 %rd193, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r27, 14;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd196;
	shf.l.wrap.b32 vl, tl, th, %r27;
	shf.l.wrap.b32 vh, th, tl, %r27;
	setp.lt.u32 p, %r27, 32;
	@!p mov.b64 %rd195, {vl,vh};
	@p  mov.b64 %rd195, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r28, 2;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd198;
	shf.l.wrap.b32 vl, tl, th, %r28;
	shf.l.wrap.b32 vh, th, tl, %r28;
	setp.lt.u32 p, %r28, 32;
	@!p mov.b64 %rd197, {vl,vh};
	@p  mov.b64 %rd197, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r29, 55;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd200;
	shf.l.wrap.b32 vl, tl, th, %r29;
	shf.l.wrap.b32 vh, th, tl, %r29;
	setp.lt.u32 p, %r29, 32;
	@!p mov.b64 %rd199, {vl,vh};
	@p  mov.b64 %rd199, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r30, 45;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd202;
	shf.l.wrap.b32 vl, tl, th, %r30;
	shf.l.wrap.b32 vh, th, tl, %r30;
	setp.lt.u32 p, %r30, 32;
	@!p mov.b64 %rd201, {vl,vh};
	@p  mov.b64 %rd201, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r31, 36;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd204;
	shf.l.wrap.b32 vl, tl, th, %r31;
	shf.l.wrap.b32 vh, th, tl, %r31;
	setp.lt.u32 p, %r31, 32;
	@!p mov.b64 %rd203, {vl,vh};
	@p  mov.b64 %rd203, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r32, 28;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd206;
	shf.l.wrap.b32 vl, tl, th, %r32;
	shf.l.wrap.b32 vh, th, tl, %r32;
	setp.lt.u32 p, %r32, 32;
	@!p mov.b64 %rd205, {vl,vh};
	@p  mov.b64 %rd205, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r33, 21;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd208;
	shf.l.wrap.b32 vl, tl, th, %r33;
	shf.l.wrap.b32 vh, th, tl, %r33;
	setp.lt.u32 p, %r33, 32;
	@!p mov.b64 %rd207, {vl,vh};
	@p  mov.b64 %rd207, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r34, 15;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd210;
	shf.l.wrap.b32 vl, tl, th, %r34;
	shf.l.wrap.b32 vh, th, tl, %r34;
	setp.lt.u32 p, %r34, 32;
	@!p mov.b64 %rd209, {vl,vh};
	@p  mov.b64 %rd209, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r35, 10;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd212;
	shf.l.wrap.b32 vl, tl, th, %r35;
	shf.l.wrap.b32 vh, th, tl, %r35;
	setp.lt.u32 p, %r35, 32;
	@!p mov.b64 %rd211, {vl,vh};
	@p  mov.b64 %rd211, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r36, 6;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd214;
	shf.l.wrap.b32 vl, tl, th, %r36;
	shf.l.wrap.b32 vh, th, tl, %r36;
	setp.lt.u32 p, %r36, 32;
	@!p mov.b64 %rd213, {vl,vh};
	@p  mov.b64 %rd213, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r37, 3;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd216;
	shf.l.wrap.b32 vl, tl, th, %r37;
	shf.l.wrap.b32 vh, th, tl, %r37;
	setp.lt.u32 p, %r37, 32;
	@!p mov.b64 %rd215, {vl,vh};
	@p  mov.b64 %rd215, {vh,vl};
	}

	// end inline asm
	not.b64 	%rd238, %rd171;
	and.b64  	%rd239, %rd183, %rd238;
	xor.b64  	%rd240, %rd239, %rd237;
	not.b64 	%rd241, %rd183;
	and.b64  	%rd242, %rd207, %rd241;
	xor.b64  	%rd334, %rd242, %rd171;
	not.b64 	%rd243, %rd207;
	and.b64  	%rd244, %rd195, %rd243;
	xor.b64  	%rd329, %rd183, %rd244;
	not.b64 	%rd245, %rd195;
	and.b64  	%rd246, %rd237, %rd245;
	xor.b64  	%rd324, %rd207, %rd246;
	not.b64 	%rd247, %rd237;
	and.b64  	%rd248, %rd171, %rd247;
	xor.b64  	%rd319, %rd195, %rd248;
	not.b64 	%rd249, %rd173;
	and.b64  	%rd250, %rd215, %rd249;
	xor.b64  	%rd338, %rd250, %rd205;
	not.b64 	%rd251, %rd215;
	and.b64  	%rd252, %rd201, %rd251;
	xor.b64  	%rd333, %rd252, %rd173;
	not.b64 	%rd253, %rd201;
	and.b64  	%rd254, %rd175, %rd253;
	xor.b64  	%rd328, %rd215, %rd254;
	not.b64 	%rd255, %rd175;
	and.b64  	%rd256, %rd205, %rd255;
	xor.b64  	%rd323, %rd201, %rd256;
	not.b64 	%rd257, %rd205;
	and.b64  	%rd258, %rd173, %rd257;
	xor.b64  	%rd318, %rd175, %rd258;
	not.b64 	%rd259, %rd213;
	and.b64  	%rd260, %rd185, %rd259;
	xor.b64  	%rd337, %rd260, %rd169;
	not.b64 	%rd261, %rd185;
	and.b64  	%rd262, %rd187, %rd261;
	xor.b64  	%rd332, %rd262, %rd213;
	not.b64 	%rd263, %rd187;
	and.b64  	%rd264, %rd179, %rd263;
	xor.b64  	%rd327, %rd185, %rd264;
	not.b64 	%rd265, %rd179;
	and.b64  	%rd266, %rd169, %rd265;
	xor.b64  	%rd322, %rd187, %rd266;
	not.b64 	%rd267, %rd169;
	and.b64  	%rd268, %rd213, %rd267;
	xor.b64  	%rd317, %rd179, %rd268;
	not.b64 	%rd269, %rd203;
	and.b64  	%rd270, %rd211, %rd269;
	xor.b64  	%rd336, %rd270, %rd193;
	not.b64 	%rd271, %rd211;
	and.b64  	%rd272, %rd209, %rd271;
	xor.b64  	%rd331, %rd272, %rd203;
	not.b64 	%rd273, %rd209;
	and.b64  	%rd274, %rd189, %rd273;
	xor.b64  	%rd326, %rd211, %rd274;
	not.b64 	%rd275, %rd189;
	and.b64  	%rd276, %rd193, %rd275;
	xor.b64  	%rd321, %rd209, %rd276;
	not.b64 	%rd277, %rd193;
	and.b64  	%rd278, %rd203, %rd277;
	xor.b64  	%rd316, %rd189, %rd278;
	not.b64 	%rd279, %rd199;
	and.b64  	%rd280, %rd177, %rd279;
	xor.b64  	%rd335, %rd280, %rd181;
	not.b64 	%rd281, %rd177;
	and.b64  	%rd282, %rd191, %rd281;
	xor.b64  	%rd330, %rd282, %rd199;
	not.b64 	%rd283, %rd191;
	and.b64  	%rd284, %rd197, %rd283;
	xor.b64  	%rd325, %rd177, %rd284;
	not.b64 	%rd285, %rd197;
	and.b64  	%rd286, %rd181, %rd285;
	xor.b64  	%rd320, %rd191, %rd286;
	not.b64 	%rd287, %rd181;
	and.b64  	%rd288, %rd199, %rd287;
	xor.b64  	%rd315, %rd197, %rd288;
	ld.const.u64 	%rd289, [%rd314];
	xor.b64  	%rd339, %rd240, %rd289;
	add.s64 	%rd314, %rd314, 8;
	add.s32 	%r79, %r79, 1;
	setp.ne.s32 	%p5, %r79, 24;
	@%p5 bra 	$L__BB2_2;

	shr.u64 	%rd290, %rd339, 16;
	cvt.u32.u64 	%r38, %rd339;
	shr.u64 	%rd291, %rd339, 32;
	shr.u64 	%rd292, %rd339, 40;
	cvt.u32.u64 	%r39, %rd292;
	shr.u64 	%rd293, %rd339, 48;
	shr.u64 	%rd294, %rd339, 56;
	shr.u64 	%rd295, %rd334, 16;
	cvt.u32.u64 	%r40, %rd334;
	shr.u64 	%rd296, %rd334, 32;
	shr.u64 	%rd297, %rd334, 40;
	cvt.u32.u64 	%r41, %rd297;
	shr.u64 	%rd298, %rd334, 48;
	shr.u64 	%rd299, %rd334, 56;
	shr.u64 	%rd300, %rd329, 16;
	cvt.u32.u64 	%r42, %rd329;
	shr.u64 	%rd301, %rd329, 32;
	shr.u64 	%rd302, %rd329, 40;
	cvt.u32.u64 	%r43, %rd302;
	shr.u64 	%rd303, %rd329, 48;
	shr.u64 	%rd304, %rd329, 56;
	shr.u64 	%rd305, %rd324, 56;
	shr.u64 	%rd306, %rd324, 48;
	shr.u64 	%rd307, %rd324, 40;
	cvt.u32.u64 	%r44, %rd307;
	shr.u64 	%rd308, %rd324, 32;
	cvt.u32.u64 	%r45, %rd324;
	shr.u64 	%rd309, %rd324, 16;
	cvt.u16.u64 	%rs1, %rd305;
	cvt.u16.u64 	%rs2, %rd306;
	shl.b16 	%rs3, %rs2, 8;
	or.b16  	%rs4, %rs1, %rs3;
	cvt.u32.u64 	%r46, %rd308;
	and.b32  	%r47, %r44, 255;
	prmt.b32 	%r48, %r46, %r47, 30212;
	cvt.u16.u32 	%rs5, %r48;
	cvt.u16.u64 	%rs6, %rd304;
	cvt.u16.u64 	%rs7, %rd303;
	shl.b16 	%rs8, %rs7, 8;
	or.b16  	%rs9, %rs6, %rs8;
	cvt.u32.u64 	%r49, %rd301;
	and.b32  	%r50, %r43, 255;
	prmt.b32 	%r51, %r49, %r50, 30212;
	cvt.u16.u32 	%rs10, %r51;
	cvt.u16.u64 	%rs11, %rd324;
	shl.b16 	%rs12, %rs11, 8;
	shr.u16 	%rs13, %rs11, 8;
	or.b16  	%rs14, %rs13, %rs12;
	shr.u32 	%r52, %r45, 24;
	cvt.u32.u64 	%r53, %rd309;
	prmt.b32 	%r54, %r53, %r52, 30212;
	cvt.u16.u32 	%rs15, %r54;
	cvt.u16.u64 	%rs16, %rd329;
	shl.b16 	%rs17, %rs16, 8;
	shr.u16 	%rs18, %rs16, 8;
	or.b16  	%rs19, %rs18, %rs17;
	shr.u32 	%r55, %r42, 24;
	cvt.u32.u64 	%r56, %rd300;
	prmt.b32 	%r57, %r56, %r55, 30212;
	cvt.u16.u32 	%rs20, %r57;
	mov.b32 	%r58, {%rs20, %rs19};
	mov.b32 	%r59, {%rs15, %rs14};
	mov.b32 	%r60, {%rs9, %rs10};
	mov.b32 	%r61, {%rs4, %rs5};
	st.local.v4.u32 	[%rd10], {%r61, %r59, %r60, %r58};
	cvt.u16.u64 	%rs21, %rd299;
	cvt.u16.u64 	%rs22, %rd298;
	shl.b16 	%rs23, %rs22, 8;
	or.b16  	%rs24, %rs21, %rs23;
	cvt.u32.u64 	%r62, %rd296;
	and.b32  	%r63, %r41, 255;
	prmt.b32 	%r64, %r62, %r63, 30212;
	cvt.u16.u32 	%rs25, %r64;
	cvt.u16.u64 	%rs26, %rd294;
	cvt.u16.u64 	%rs27, %rd293;
	shl.b16 	%rs28, %rs27, 8;
	or.b16  	%rs29, %rs26, %rs28;
	cvt.u32.u64 	%r65, %rd291;
	and.b32  	%r66, %r39, 255;
	prmt.b32 	%r67, %r65, %r66, 30212;
	cvt.u16.u32 	%rs30, %r67;
	cvt.u16.u64 	%rs31, %rd334;
	shl.b16 	%rs32, %rs31, 8;
	shr.u16 	%rs33, %rs31, 8;
	or.b16  	%rs34, %rs33, %rs32;
	shr.u32 	%r68, %r40, 24;
	cvt.u32.u64 	%r69, %rd295;
	prmt.b32 	%r70, %r69, %r68, 30212;
	cvt.u16.u32 	%rs35, %r70;
	cvt.u16.u64 	%rs36, %rd339;
	shl.b16 	%rs37, %rs36, 8;
	shr.u16 	%rs38, %rs36, 8;
	or.b16  	%rs39, %rs38, %rs37;
	shr.u32 	%r71, %r38, 24;
	cvt.u32.u64 	%r72, %rd290;
	prmt.b32 	%r73, %r72, %r71, 30212;
	cvt.u16.u32 	%rs40, %r73;
	mov.b32 	%r74, {%rs40, %rs39};
	mov.b32 	%r75, {%rs35, %rs34};
	mov.b32 	%r76, {%rs29, %rs30};
	mov.b32 	%r77, {%rs24, %rs25};
	st.local.v4.u32 	[%rd10+16], {%r77, %r75, %r76, %r74};
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd158;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd68;
	.param .b32 retval0;
	call.uni (retval0), 
	_ZN39_INTERNAL_467e079b_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1_, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r78, [retval0+0];
	} // callseq 4
	cvt.u16.u32 	%rs41, %r78;
	setp.eq.s16 	%p6, %rs41, 0;
	@%p6 bra 	$L__BB2_6;

	mov.u64 	%rd340, 0;

$L__BB2_5:
	add.s64 	%rd312, %rd1, %rd340;
	ld.u8 	%rs42, [%rd312];
	add.s64 	%rd313, %rd11, %rd340;
	st.global.u8 	[%rd313], %rs42;
	add.s64 	%rd340, %rd340, 1;
	setp.lt.u64 	%p7, %rd340, 32;
	@%p7 bra 	$L__BB2_5;

$L__BB2_6:
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 5

$L__BB2_7:
	ret;

}

